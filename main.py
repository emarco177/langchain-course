from langchain_groq import ChatGroq
from langchain_ollama import ChatOllama
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import os


def main():
    # 1Ô∏è‚É£ Carga variables de entorno (.env)
    load_dotenv()

    print("üöÄ Hello from langchain-course! - LangChain Agents")

    # 2Ô∏è‚É£ Definir el prompt base
    information = "elon musk information"

    summary_template = """Given the information {information} about a person,
    create:
    1. A short summary.
    2. A fun fact about them.
    """

    summary_prompt_template = PromptTemplate(
        input_variables=["information"],
        template=summary_template
    )

    # 3Ô∏è‚É£ Detectar si hay API Key de Groq (si no, usar Ollama)
    groq_api_key = os.getenv("GROQ_API_KEY")

    if groq_api_key:
        print("‚ö° Using Groq API (cloud model)")
        llm = ChatGroq(
            model="llama-3.3-70b-versatile",  # ‚úÖ modelo Groq m√°s reciente
            api_key=groq_api_key,
            temperature=0.3
        )
    else:
        print("üß† Using Ollama (local model)")
        llm = ChatOllama(
            model="gemma3:1b",  # ‚úÖ modelo que ya descargaste
            temperature=0.7
        )

    # 4Ô∏è‚É£ Conectar el prompt con el modelo
    chain = summary_prompt_template | llm

    # 5Ô∏è‚É£ Ejecutar
    response = chain.invoke({"information": information})

    print("\nü§ñ Respuesta del agente:\n")
    print(response.content)


if __name__ == "__main__":
    main()
